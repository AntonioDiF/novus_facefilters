<!DOCTYPE html>
<html lang="es">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Real-Time Facial Emotion Detection</title>
    <link rel="stylesheet" href="https://rsms.me/inter/inter.css" />
    <link rel="stylesheet" href="style.css" />
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.4.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@0.0.1/dist/face-landmarks-detection.js"></script>
  </head>
  <body>
    <div class="header">
      <h1>Sensorial Mind</h1>
      <canvas id="output"></canvas>
    </div>
    <div class="body">
      <div>
        <video id="mora" width="960" height="720">
          <source src="test_video.mp4" type="video/mp4" />
          Your browser does not support the <code>video</code> element. Please
          use the last version Chrome or Firefox.
        </video>
      </div>
      <button id="play_button" class="button" disabled>Play</button>
      <h1 id="status" class="hidden">Loading...</h1>
      <video id="webcam" playsinline class="webcam"></video>
    </div>
    <script>
      function setText(text) {
        document.getElementById('status').innerText = text;
      }

      function drawLine(ctx, x1, y1, x2, y2) {
        ctx.beginPath();
        ctx.moveTo(x1, y1);
        ctx.lineTo(x2, y2);
        ctx.stroke();
      }

      async function setupWebcam() {
        return new Promise((resolve, reject) => {
          const webcamElement = document.getElementById('webcam');
          const navigatorAny = navigator;
          navigator.getUserMedia =
            navigator.getUserMedia ||
            navigatorAny.webkitGetUserMedia ||
            navigatorAny.mozGetUserMedia ||
            navigatorAny.msGetUserMedia;
          if (navigator.getUserMedia) {
            navigator.getUserMedia(
              { video: true },
              (stream) => {
                webcamElement.srcObject = stream;
                webcamElement.addEventListener('loadeddata', resolve, false);
              },
              (error) => reject()
            );
          } else {
            reject();
          }
        });
      }

      const emotions = [
        'angry',
        'disgust',
        'fear',
        'happy',
        'neutral',
        'sad',
        'surprise',
      ];
      let emotionModel = null;

      let output = null;
      let model = null;

      let logging = false;
      let logging_out = 'data:text/csv;charset=utf-8,';
      let logging_base_time = window.performance.now();

      async function predictEmotion(points) {
        let result = tf.tidy(() => {
          const xs = tf.stack([tf.tensor1d(points)]);
          return emotionModel.predict(xs);
        });
        let prediction = await result.data();
        result.dispose();
        // Get the index of the maximum value
        // let id = prediction.indexOf( Math.max( ...prediction ) );
        // return emotions[ id ];
        let i = 0;
        let emotions_obj = emotions.map((emotion) => {
          i += 1;
          return `${prediction[i - 1]}`; //`${emotion}: ${prediction[i - 1]}`
        });
        return emotions_obj;
      }

      async function trackFace() {
        const video = document.getElementById('webcam');
        const faces = await model.estimateFaces({
          input: video,
          returnTensors: false,
          flipHorizontal: false,
        });
        output.drawImage(
          video,
          0,
          0,
          video.width,
          video.height,
          0,
          0,
          video.width,
          video.height
        );

        let points = null;
        faces.forEach((face) => {
          // Draw the bounding box
          const x1 = face.boundingBox.topLeft[0];
          const y1 = face.boundingBox.topLeft[1];
          const x2 = face.boundingBox.bottomRight[0];
          const y2 = face.boundingBox.bottomRight[1];
          const bWidth = x2 - x1;
          const bHeight = y2 - y1;
          drawLine(output, x1, y1, x2, y1);
          drawLine(output, x2, y1, x2, y2);
          drawLine(output, x1, y2, x2, y2);
          drawLine(output, x1, y1, x1, y2);

          // Add just the nose, cheeks, eyes, eyebrows & mouth
          const features = [
            'noseTip',
            'leftCheek',
            'rightCheek',
            'leftEyeLower1',
            'leftEyeUpper1',
            'rightEyeLower1',
            'rightEyeUpper1',
            'leftEyebrowLower', //"leftEyebrowUpper",
            'rightEyebrowLower', //"rightEyebrowUpper",
            'lipsLowerInner', //"lipsLowerOuter",
            'lipsUpperInner', //"lipsUpperOuter",
          ];
          points = [];
          features.forEach((feature) => {
            face.annotations[feature].forEach((x) => {
              points.push((x[0] - x1) / bWidth);
              points.push((x[1] - y1) / bHeight);
            });
          });
        });

        if (points) {
          let emotion = await predictEmotion(points);
          // setText( `Detected: ${emotion}` );
          if (logging) {
            logging_out =
              logging_out +
              (window.performance.now() - logging_base_time) +
              ',' +
              emotion +
              '\n';
          }
        } else {
          setText('No Face');
        }

        requestAnimationFrame(trackFace);
      }

      (async () => {
        await setupWebcam();
        const video = document.getElementById('webcam');
        video.play();
        let videoWidth = video.videoWidth;
        let videoHeight = video.videoHeight;
        video.width = videoWidth;
        video.height = videoHeight;

        let canvas = document.getElementById('output');
        const DIVISOR = video.height / 100; // setea la altura del video de salida a 200px
        canvas.width = video.width / DIVISOR;
        canvas.height = video.height / DIVISOR;

        output = canvas.getContext('2d');
        output.translate(canvas.width, 0);
        output.scale(-1 / DIVISOR, 1 / DIVISOR); // Mirror cam
        output.fillStyle = '#fdffb6';
        output.strokeStyle = '#fdffb6';
        output.lineWidth = 2;

        // Load Face Landmarks Detection
        model = await faceLandmarksDetection.load(
          faceLandmarksDetection.SupportedPackages.mediapipeFacemesh
        );
        // Load Emotion Detection
        emotionModel = await tf.loadLayersModel('web/model/facemo.json');

        setText('Loaded!');
        document.getElementById('play_button').disabled = false;
        document.getElementById('play_button').style.visibility = 'visible';

        trackFace();
      })();

      function start_logging() {
        const video = document.getElementById('mora');
        video.play();
        logging_out = 'data:text/csv;charset=utf-8,';
        logging_base_time = window.performance.now();
        logging = true;
        document.getElementById('play_button').disabled = true;
        document.getElementById('play_button').style.visibility = 'hidden';

        // Esperar 1 minuto y 53 segundos
        setTimeout(() => {
          logging = false;
          console.log(logging_out);
          video.pause();

          let encodedUri = encodeURI(logging_out);
          var link = document.createElement('a');
          link.setAttribute('href', encodedUri);
          link.setAttribute('download', 'data_novus.csv');
          document.body.appendChild(link); // Required for Firefox

          link.click();
        }, 113000);
      }

      document
        .getElementById('play_button')
        .addEventListener('click', start_logging, false);
    </script>
  </body>
</html>
